{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Weekend Effect"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Introduction\n",
      "        \n",
      "The Weekend Effect is defined as, \"A phenomenon in financial markets in which stock returns on Mondays are often significantly lower than those of the immediate preceding Fridays.\" Some theories that explain the effect attribute the tendency for companies to release bad news on Friday after the markets close to depressed stock prices on Monday. However, the others do not acknowledge such weekly tendencies and stated that it may be a pure stochastic event.\n",
      "        \n",
      "In this module, we intend to employ a series of conventional statistical methods to analyze whether or not the stock market, resembled by the SP500 Index, exhibited a tendency against the weekend evenings. Our intention is to change minds of the traders who believe in the Weekend Effect.\n",
      "        \n",
      "To begin, we set the workspace, as well as retrive data locally stored in the work space. Immediately next, we want to split the historical SP500 data by the days of the week. Hence, append a new column in the SP500 dataframe, and denote it \"Day.\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "print os.getcwd()\n",
      "os.chdir(\"D:\\Dropbox\\UC Berkeley\\spring\\STAT222\")\n",
      "\n",
      "# or elsewhere at which \"daily.csv\" is stored"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/edwsurewin/Dropbox/Berkeley MA/222/STAT222-SP500\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "SP500 = pd.read_csv(\"daily.csv\")\n",
      "SP500['Day'] = None # create a Day of the Week column\n",
      "print(SP500.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         Date     Open     High      Low    Close      Volume  Adj Close   Day\n",
        "0  2014-02-28  1855.12  1867.92  1847.67  1859.45  3917450000    1859.45  None\n",
        "1  2014-02-27  1844.90  1854.53  1841.13  1854.29  3547460000    1854.29  None\n",
        "2  2014-02-26  1845.79  1852.65  1840.66  1845.16  3716730000    1845.16  None\n",
        "3  2014-02-25  1847.66  1852.91  1840.19  1845.12  3515560000    1845.12  None\n",
        "4  2014-02-24  1836.78  1858.71  1836.78  1847.61  4014530000    1847.61  None\n",
        "\n",
        "[5 rows x 8 columns]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quantity of interest\n",
      "\n",
      "First of all, we proceed by defining a quantity of interest. Since we are interested in the overnight changes, we are interested in the overnight returns by days of the week. Denote,\n",
      "                                     \n",
      "<center> overnight return $R_{today}= \\frac{Next-Trading-Day-Opening}{Today's-Adjusted-Close}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Therefore, during any given ordinary week without holidays (i.e. non-trading days), we have five overnight returns. Next, we defined a function that automates computing the rate of return for any two consecutive trading days. Then, we employed a built-in package in python, which calculates the \"day of the week\" indicator for each row observation. \n",
      "        \n",
      "Once we had the returns computed together with the indicators, we construct five dynamic lists to store Monday, Tuesday, Wednesday, Thursday, and Friday overnight returns, respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print(SP500.tail()) = 0 to 16146\n",
      "# len(SP500['Day']) = 16147`\n",
      "\n",
      "rate = [None]*(len(SP500['Day'])-1)\n",
      "\n",
      "#create an array and fill in \n",
      "def compute_rate(SP500, rate):\n",
      "    for i in range(0, len(SP500['Day'])-1):\n",
      "        rate[i] = SP500['Open'][i]/SP500['Adj Close'][i+1]\n",
      "    return rate\n",
      "\n",
      "rate = compute_rate(SP500, rate)\n",
      "\n",
      "# first 5 items in open/close rate list\n",
      "print(rate[0:5]) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1.0004476106757842, 0.99985909081055302, 1.0003631200138745, 1.0000270619881904, 1.0002886317222601]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import datetime\n",
      "\n",
      "# fill out the days of the weeks\n",
      "# 1: Monday, 2: Tuesday, ..., 5: Friday\n",
      "\n",
      "for i in range(0, len(SP500['Day'])):   \n",
      "    SP500['Day'][i] = datetime.date(int(SP500['Date'][i][0:4]), int(SP500['Date'][i][5:7]), int(SP500['Date'][i][8:10])).isoweekday()\n",
      "\n",
      "print(SP500.head())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         Date     Open     High      Low    Close      Volume  Adj Close Day\n",
        "0  2014-02-28  1855.12  1867.92  1847.67  1859.45  3917450000    1859.45   5\n",
        "1  2014-02-27  1844.90  1854.53  1841.13  1854.29  3547460000    1854.29   4\n",
        "2  2014-02-26  1845.79  1852.65  1840.66  1845.16  3716730000    1845.16   3\n",
        "3  2014-02-25  1847.66  1852.91  1840.19  1845.12  3515560000    1845.12   2\n",
        "4  2014-02-24  1836.78  1858.71  1836.78  1847.61  4014530000    1847.61   1\n",
        "\n",
        "[5 rows x 8 columns]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create dynamic lists to store overnight returns of corresponding days of the week\n",
      "# control group  Friday, whereas experiment group  any other day(s) of the week\n",
      "\n",
      "# e.g. a Monday's overnight is defined to be \n",
      "#      (the quotient of) Tuesday Opening / Monday's Adj Close\n",
      "\n",
      "Monday = []       \n",
      "Tuesday = []      \n",
      "Wednesday = []    \n",
      "Thursday = []     \n",
      "Friday = []       \n",
      "\n",
      "for i in range(1, len(SP500['Day'])):\n",
      "    if SP500['Day'][i] == 1:\n",
      "        Monday.append(rate[i-1])\n",
      "    else:\n",
      "        if SP500['Day'][i] == 2:\n",
      "            Tuesday.append(rate[i-1])\n",
      "        else:\n",
      "            if SP500['Day'][i] == 3:\n",
      "                Wednesday.append(rate[i-1])\n",
      "            else:\n",
      "                if SP500['Day'][i] == 4:\n",
      "                    Thursday.append(rate[i-1])\n",
      "                else:\n",
      "                    if SP500['Day'][i] == 5:\n",
      "                        Friday.append(rate[i-1])   \n",
      "# recent 10 years\n",
      "Monday = Monday[0:520]      \n",
      "Tuesday = Tuesday[0:520]      \n",
      "Wednesday = Wednesday[0:520]     \n",
      "Thursday = Thursday[0:520]       \n",
      "Friday = Friday[0:520]     \n",
      "                \n",
      "# for exmaple, Friday vs. NonFriday\n",
      "NonFri = Monday + Tuesday + Wednesday + Thursday"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Quantity of Interest Plots\n",
      "\n",
      "Before moving on, we subset the first 520 elements in each list; therefore, we have roughly the recent 10-year data to analyze upon. The reason of subsetting is that the fundamentals of stock market might have changed substantially over the last decades, and historical data from the past could tell little about the current dynamics in today's market. We want to rule out the noises due to a different financial regulation settings happenend in the past, and want to solely pinpoint what's happened recently. \n",
      "        \n",
      "With the data cleaned and sort, we would like to have a visual impression how the data look like, and whether we could draw any immediate preliminary conclusion from the pictures. We plotted the following 4 pictures, with each of the Friday group and another day of the week. Notice that the Friday group is the control group, and 4 other groups are the treatment groups; in other words, we apply another day of the week and try to observe possiblity of significance improvements.\n",
      "        \n",
      "On the x-axis, we have \"chronological number of weeks until March 6th, 2014\", when we lastly accessed the public data from Yahoo Faince. On the y-axis, we have the quantitiy of interest being plotted, i.e. the overnight rates of return. The red dots are of the Friday group, the black dots are of the respective treatement groups. We attempt to answer these questions:\n",
      "        \n",
      "1) Are all the red dots systematically lower than the black dots?\n",
      "2) If so, which picture shows the most apparent difference?\n",
      "3) Any patterns?\n",
      "            \n",
      "To answer these questions, we take a closer look at each picture. It seems that the answers for the first two questions are negative, while we can say something about the 3). It appears that we experienced a period of volatilities during Week 100 to Week 300; if we convert the dates back to calendar, we will notice that that was the period we were in the financial meltdown commercing 2007. Hence, the pictures nevertheless showed we caputured the data correctly.\n",
      "        \n",
      "Yet, we are not able to make any statements with regards to the research objective. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import matplotlib\n",
      "\n",
      "# overnight return: Monday vs. Friday\n",
      "# X-axis is timeline in chronological order\n",
      "\n",
      "plt.plot(Monday, 'ko', label='Monday')\n",
      "plt.hold(True)\n",
      "plt.plot(Friday, 'ro', label='Friday', alpha=0.7)\n",
      "plt.legend(loc=0)\n",
      "plt.title('Overnight Return Comparison, Fri vs. Mon')\n",
      "plt.ylabel('Overnight Return')\n",
      "plt.xlabel('Chronological # of Weeks until Mar 06 2014')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'plt' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-cf6dc48bf2fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X-axis is timeline in chronological order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ko'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Monday'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFriday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Friday'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# overnight return: Tuesday vs. Friday\n",
      "# X-axis is timeline in chronological order\n",
      "\n",
      "plt.plot(Tuesday, 'ko', label='Tuesday')\n",
      "plt.hold(True)\n",
      "plt.plot(Friday, 'ro', label='Friday', alpha=0.7)\n",
      "plt.title('Overnight Return Comparison, Fri vs. Tue')\n",
      "plt.ylabel('Overnight Return')\n",
      "plt.xlabel('Chronological # of Weeks until Mar 06 2014')\n",
      "plt.legend(loc=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# overnight return: Wednesday vs. Friday\n",
      "# X-axis is timeline in chronological order\n",
      "\n",
      "plt.plot(Wednesday, 'ko', label='Wednesday')\n",
      "plt.hold(True)\n",
      "plt.plot(Friday, 'ro', label='Friday', alpha=0.7)\n",
      "plt.title('Overnight Return Comparison, Fri vs. Wed')\n",
      "plt.ylabel('Overnight Return')\n",
      "plt.xlabel('Chronological # of Weeks until Mar 06 2014')\n",
      "plt.legend(loc=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# overnight return: Thursday vs. Friday\n",
      "# X-axis is timeline in chronological order\n",
      "\n",
      "plt.plot(Thursday, 'ko', label='Thursday')\n",
      "plt.hold(True)\n",
      "plt.plot(Friday, 'ro', label='Friday', alpha=0.7)\n",
      "plt.title('Overnight Return Comparison, Fri vs. Thurs')\n",
      "plt.ylabel('Overnight Return')\n",
      "plt.xlabel('Chronological # of Weeks until Mar 06 2014')\n",
      "plt.legend(loc=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Naturally, we are going to empower our research by performing a series of statistical tests, wherever applicable. Notice that most tests have a distinct set of requirements under which the data have to suffice. For the time being, we are willing to assume all required, although that could lead us to an aggressive concludsion."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Mann\u2013Whitney U Test \n",
      "\n",
      "More efficient than the T test in non-normal distribution, this test is said to embrace a very general formulation which assumes that:\n",
      "\n",
      "1.\tAll the observations from both groups are independent of each other,\n",
      "2.\tThe responses are ordinal (i.e. one can at least say, of any two observations, which is the greater),\n",
      "3.\tThe distributions of both groups are equal under the null hypothesis, so that the probability of an observation from one population (X) exceeding an observation from the second population (Y) equals the probability of an observation from Y exceeding an observation from X. That is, there is a symmetry between populations with respect to probability of random drawing of a larger observation.\n",
      "4.\tUnder the alternative hypothesis, the probability of an observation from one population (X) exceeding an observation from the second population (Y) (after exclusion of ties) is not equal to 0.5. The alternative may also be stated in terms of a one-sided test, for example: P(X\u00a0>\u00a0Y)\u00a0+\u00a00.5 P(X\u00a0=\u00a0Y) \u00a0is greater than\u00a00.5.\n",
      "    \n",
      "Thus, null is mean(Friday) = mean(nonFriday), and inequality otherwise.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats as stats\n",
      "u, MW_p_value = stats.mannwhitneyu(Friday, NonFri)\n",
      "print(\"two-sample wilcoxon-test p-value is\", MW_p_value)\n",
      "\n",
      "# p_value < 0.01 => alternative hypothesis:\n",
      "# they don't have the same mean at the 1% significance level"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Paired T-test\n",
      "\n",
      "The test assumes that the differences between pairs are normally distributed; one can use the histogram spreadsheet on that page to check the normality. The null hypothesis is that the mean difference between paired observations is zero. In other words,\n",
      "        \n",
      "Null: mean(Friday) - mean(nonFriday) = 0, and\n",
      "Alternative: Otherwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AvgNon = [None]*len(Friday)\n",
      "for i in range(0, len(AvgNon)):\n",
      "    AvgNon[i] = (Monday[i]+Tuesday[i]+Wednesday[i]+Thursday[i])/4\n",
      "\n",
      "PT_diff = [None]*len(Friday)  \n",
      "for i in range(0, len(AvgNon)):\n",
      "    PT_diff[i] = Friday[i] - AvgNon[i]\n",
      "\n",
      "PT_tstats=((mean(Friday)-mean(AvgNon))-0)/(std(PT_diff)*sqrt(len(Friday)-1))\n",
      "\n",
      "print(\"Matched Pair T Test yields t-stat\", PT_tstats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t_statistic, TSTAT_p_value = stats.ttest_ind(Friday, NonFri)\n",
      "print(\"similarly in two-sample t-test, p-value is\", TSTAT_p_value)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Mean of Fridays is\", mean(Friday), \"and mean of NonFris is\", mean(NonFri))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The Kruskal-Wallis H-test \n",
      "\n",
      "This test exams the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. Therefore,\n",
      "        \n",
      "                \n",
      "Null: medium(Friday) - medium(nonFriday) = 0, and\n",
      "Alternative: Otherwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u, ANOVA_p_value = stats.mstats.kruskalwallis(Friday, NonFri) \n",
      "# Chi-square degree of freedom 1\n",
      "print(\"one-way ANOVA Kruskal Wallis test p-value is\", ANOVA_p_value)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With a p value smaller than 0.01, we are able to claim that the alternative hypothesis: The groups do not have the same median at the 1% significance level. As a hindsight remark, the Mann\u2013Whitney U Test and the Kruskal-Wallis H-test suffer from a limitation that, the Friday grouop might not be independent from the Non-friday group, especially given the data is time-series."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##The Friedman Test \n",
      "        \n",
      "This test exames the null hypothesis that repeated measurements of the same individuals have the same distribution. However, it additionally requires the groups have are paired and thus have the same size, which our data satisfies."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# In order to apply Friedman test, measurements must have equal length, hence,\n",
      "# ASSUME that all the missing days are of no information\n",
      "\n",
      "#FridayFM = Friday + [NaN]*63\n",
      "#MondayFM = Monday + [NaN]*187\n",
      "#TuesdayFM = Tuesday + [NaN]*1\n",
      "#WednesdayFM = Wednesday\n",
      "#ThursdayFM = Thursday + [NaN]*43\n",
      "\n",
      "FridayFM = Friday\n",
      "MondayFM = Monday \n",
      "TuesdayFM = Tuesday \n",
      "WednesdayFM = Wednesday\n",
      "ThursdayFM = Thursday \n",
      "\n",
      "print(len(FridayFM), len(MondayFM), len(TuesdayFM), len(WednesdayFM), len(ThursdayFM))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u, fridman_p_value=stats.mstats.friedmanchisquare(FridayFM, MondayFM, TuesdayFM, WednesdayFM, ThursdayFM)\n",
      "print(\"five-measure Friedman test p-value is\", fridman_p_value)\n",
      "\n",
      "# p_value < 0.01 => alternative hypothesis:\n",
      "# some of the measurements were taken differently at the 1% significance level"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u, p_value = stats.mstats.friedmanchisquare(MondayFM, TuesdayFM, WednesdayFM, ThursdayFM)\n",
      "print(\"four-measure (excluding Friday) Friedman test p-value is\", p_value)\n",
      "\n",
      "# p_value > 0.01 => null hypothesis:\n",
      "# null is not rejected at the 1% significance level, measurements consistant"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The limitation of Friedman Test is that, due to the assumption that the test statistic has a chi squared distribution, the p value is only reliable for n greater than 10 and more than 6 repeated measurements. However, we only as well as could at most have 4 repeated measurements."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Statistical Tests Result Table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# put results in a table\n",
      "\n",
      "data = {'Test Name': ['Wilcoxon rank', 'Matched Pair T', 'Kruskal-Wallis', 'T Statistic', 'Fridman'],\n",
      "        'Requirements': ['independent, dist equal under null', 'paired up, normal', 'one-way ANOVA, iid, normal', 'student\"s t-tests, dof > 30', 'repeated measure same dist?'],\n",
      "        'P Value': [MW_p_value, PT_tstats, ANOVA_p_value, TSTAT_p_value, fridman_p_value],\n",
      "        'Signif': [\">.05\", \"<.01\",\">.05\", \">.05\", \"<.01\"]}\n",
      "football = pd.DataFrame(data, columns=['Test Name', 'Requirements', 'P Value', 'Signif'])\n",
      "print (football)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Histogram\n",
      "\n",
      "Among all the tests above, only two tests came up postivie at the 5% signficance level, indicating some confidence that the Friday returns might have a different distribution pattern comparing to the rest of the groups. Again, this observation is drawn under the assumptions that all the required premises were met, unique to each individual test.\n",
      "        \n",
      "To better visually assess the how Friday group behaves in comparison to the other groups, we plotted the histogram below. From which, we are able to make reference on mean, medium and mode of each group. The historgrams below are standardized, between the Friday group and the non-friday group.\n",
      "        \n",
      "Looking at the histogram of Friday, we would attempt to think that Friday in general has a smaller expected return among the days of a week. However, when we explicitly computed out the mean of Friday group, and compared that value of the non-friday group, we were bewildered that Friday group actually had a larger expected return - or mean value - however, the histogram suggested that the mode of Friday is smaller.\n",
      "        \n",
      "Then, we were confident to think that the Friday group is relatively skewed to the left, with a heavier tail at the high-end values. That perhaps was the reason that Friday group had a larger mean. We wish we could reproduce the Friday groups many times as desired, and anlyze its behaviors in a larger scale.\n",
      "\n",
      "Hence, we consider Bootstrapping methods thereafter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# By CLT, mean is asymptotically normal with var = /sigma^{2} over n\n",
      "z = (mean(Friday)-1)/sqrt(var(Friday)/len(Friday))\n",
      "\n",
      "# Null:    mean(Friday) = 1\n",
      "# Alter:   mean(Friday) != 1\n",
      "\n",
      "print(\"z-score is\", z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# By CLT, mean is asymptotically normal with var = /sigma^{2} over n\n",
      "z= (mean(Friday+NonFri)-1)/sqrt(var(Friday+NonFri)/len(Friday+NonFri))\n",
      "\n",
      "# Null:    mean(weekdays) = 1\n",
      "# Alter:   mean(weekdays) != 1\n",
      "\n",
      "print(\"z-score is\", z)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# multi normalized histogram of Friday and NonFriday\n",
      "\n",
      "#setbins = [-0.01, -0.008, -0.006, -0.004, -0.002, 0, 0.002, 0.004, 0.006, 0.008, 0.01]\n",
      "plt.hist(Friday, bins=25, label=\"Friday\",  normed=True,  alpha=.5)\n",
      "plt.hist(NonFri+Friday, bins=25, label=\"Pooled Sample\",  normed=True, alpha=.5)\n",
      "#plt.xlim((.9995,1.0005))\n",
      "plt.title('Overnight Return Comparison, Fri vs. Thurs')\n",
      "plt.ylabel('Normalized Histogram, in 1/1000 th')\n",
      "plt.xlabel('Overnight Return')\n",
      "plt.legend(loc=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Mean of Fridays is\", mean(Friday), \"and Mean of Pooled is\", mean(NonFri+Friday))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Mean of Pooled is\", mean(NonFri+Friday))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Kernel Estimation\n",
      "\n",
      "Before bootstrapping, we would need to train our kernels such that they can catch the raw data's distributions closely. In other words, we want them to be able to reproduce the raw data to the best capacity. Given the underlying distributions were highly concentrated around 1, we were not able to choose a Gaussian kernel but had to try a spike-like \"tophat\" kernel.\n",
      "        \n",
      "We start off by simulating the raw data set, and compare-and-constrat if the reproduced data collide with the original data well. We scratched new histograms, and observed that the shapes of two histograms almost perfectly resembled each other, as well as the colors blended.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KernelDensity\n",
      "kde = KernelDensity(bandwidth=.001, algorithm='auto', kernel='tophat', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "# kernel estimation with tophat as spike-like histogram\n",
      "\n",
      "# train the model\n",
      "kde.fit(Friday)\n",
      "\n",
      "# 100 sample\n",
      "ker100 = kde.sample(n_samples=1000, random_state=None)\n",
      "\n",
      "sample = []\n",
      "for i in range(0, len(ker100)):\n",
      "    sample = sample + ker100.tolist()[0]\n",
      "\n",
      "plt.hist(Friday, bins=25, label=\"Original Friday\", normed=True, alpha=.5)\n",
      "plt.hist(sample, bins=25, normed=True, label=\"Kernel Sampled Friday\", alpha=.5)  \n",
      "# settled down to Friday's\n",
      "\n",
      "plt.title('Bootstrapping on Friday, N=1000 sampled')\n",
      "plt.ylabel('Normalized Histogram, in 1/1000 th')\n",
      "plt.xlabel('Overnight Return')\n",
      "plt.legend(loc=0)\n",
      "\n",
      "# kernel estiamted collapse w/ original, so estimation worked"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KernelDensity\n",
      "kde = KernelDensity(bandwidth=.001, algorithm='auto', kernel='tophat', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "# kernel estimation with tophat as spike-like histogram\n",
      "\n",
      "# train the model\n",
      "kde.fit(Friday+NonFri)\n",
      "\n",
      "# 100 sample\n",
      "ker100 = kde.sample(n_samples=1000, random_state=None)\n",
      "\n",
      "sample = []\n",
      "for i in range(0, len(ker100)):\n",
      "    sample = sample + ker100.tolist()[0]\n",
      "\n",
      "plt.hist(Friday+NonFri, bins=25, label=\"Original Pooled\", normed=True, alpha=.5)\n",
      "plt.hist(sample, bins=25, normed=True, label=\"Kernel Sampled\", alpha=.5)  # settled down to Friday's\n",
      "plt.title('Bootstrapping on Pooled, N=1000 sampled')\n",
      "plt.ylabel('Normalized Histogram, in 1/1000 th')\n",
      "plt.xlabel('Overnight Return')\n",
      "plt.legend(loc=0)\n",
      "\n",
      "# kernel estiamted collapse w/ original, so estimation worked"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Bootstrap\n",
      "\n",
      "Then, we are able to bootstrap. We defined the bootstrap function to be the sample mean. Therefore, for each boot, we compute and store the mean of bootstrapped sample. We first plotted the bootstrapped sample means of the Friday group and the population group.\n",
      "        \n",
      "The Central Limit Theorem dictates the histogram of boostrapped means to be normal, which they are."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MonteCarloBoot = kde.sample(n_samples=1000, random_state=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BootMean = [None]*len(MonteCarloBoot)\n",
      "def bootMean(MonteCarloBoot, BootMean):\n",
      "    for i in range(0, len(MonteCarloBoot)):\n",
      "        BootMean[i] = MonteCarloBoot[i].mean()\n",
      "    return(BootMean)\n",
      "\n",
      "BootMean = bootMean(MonteCarloBoot, BootMean)\n",
      "\n",
      "# Bootstrapping with T = mean(obs)\n",
      "# v_boot = 1/B * B_sum(T - mean(T))^2\n",
      "\n",
      "Boot_var = std(BootMean)\n",
      "\n",
      "print(\"Monte Carlo Bootstrapping with mean\", mean(BootMean), \"variance\", Boot_var)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(BootMean, bins=25,normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neighbors import KernelDensity\n",
      "# kernel estimation with tophat as spike-like histogram\n",
      "\n",
      "############################ Friday ############################\n",
      "\n",
      "kdeF = KernelDensity(bandwidth=.001, algorithm='auto', kernel='tophat', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "\n",
      "kdeF.fit(Friday)\n",
      "FriBoot = kdeF.sample(n_samples=1000, random_state=None)\n",
      "\n",
      "FriBootMean = [None]*len(FriBoot)\n",
      "FriBootMean = bootMean(FriBoot, FriBootMean)\n",
      "\n",
      "############################ Thursday ############################\n",
      "\n",
      "kdeR = KernelDensity(bandwidth=.001, algorithm='auto', kernel='tophat', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "\n",
      "kdeR.fit(Thursday)\n",
      "ThrBoot = kdeR.sample(n_samples=1000, random_state=None)\n",
      "\n",
      "ThrBootMean = [None]*len(ThrBoot)\n",
      "ThrBootMean = bootMean(ThrBoot, ThrBootMean)\n",
      "\n",
      "\n",
      "############################ Wednesday ############################\n",
      "\n",
      "\n",
      "kdeW = KernelDensity(bandwidth=.001, algorithm='auto', kernel='tophat', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "\n",
      "kdeW.fit(Wednesday)\n",
      "WedBoot = kdeW.sample(n_samples=1000, random_state=None)\n",
      "\n",
      "WedBootMean = [None]*len(WedBoot)\n",
      "WedBootMean = bootMean(WedBoot, WedBootMean)\n",
      "\n",
      "############################ Tuesday ############################\n",
      "\n",
      "\n",
      "kdeT = KernelDensity(bandwidth=.001, algorithm='auto', kernel='tophat', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "\n",
      "kdeT.fit(Wednesday)\n",
      "TueBoot = kdeT.sample(n_samples=1000, random_state=None)\n",
      "\n",
      "TueBootMean = [None]*len(TueBoot)\n",
      "TueBootMean = bootMean(TueBoot, TueBootMean)\n",
      "\n",
      "############################ Monday ############################\n",
      "\n",
      "\n",
      "kdeM = KernelDensity(bandwidth=.001, algorithm='auto', kernel='tophat', metric='euclidean', atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)\n",
      "\n",
      "kdeM.fit(Monday)\n",
      "MonBoot = kdeM.sample(n_samples=1000, random_state=None)\n",
      "\n",
      "MonBootMean = [None]*len(MonBoot)\n",
      "MonBootMean = bootMean(MonBoot, MonBootMean)\n",
      "\n",
      "    \n",
      "# plt.hist(FriBootMean, bins=25,normed=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(FriBootMean, 25, label=\"Fri\", alpha=.5, normed=True)\n",
      "#plt.hist(ThrBootMean, 25, label=\"Thr\", alpha=.5, normed=True)\n",
      "#plt.hist(WedBootMean, 25, label=\"Wed\", alpha=.5, normed=True)\n",
      "#plt.hist(TueBootMean, 25, label=\"Tue\", alpha=.5, normed=True)\n",
      "#plt.hist(MonBootMean, 25, label=\"Mon\", alpha=.5,normed=True)\n",
      "#plt.hist(BootMean, 25, label=\"Pooled\", alpha=.5, normed=True)\n",
      "plt.legend(loc=0)\n",
      "plt.ticklabel_format(style='plain', axis='x', scilimits=(0,0))\n",
      "plt.title('Bootstrap on g(T) = Sample Mean')\n",
      "plt.ylabel('Normalized Histogram, in 1/10000000 th')\n",
      "plt.xlabel('Overnight Return')\n",
      "#plt.ylim((0,300000))\n",
      "pyplot.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Bootstrap Results\n",
      "\n",
      "Monte Carlo Simulation indicates that Friday has different distribution pattern from other days of the week combined. Surprisingly, the Friday group came up the most positive. \n",
      "        \n",
      "To better understand the following plots, notice that \n",
      "        \n",
      "1. There are six histograms being plotted on the same picture, but two of which overlapped. \n",
      "2. The tallest and skinest one is the boot mean of the population. As it had five times as many size of the others, its standard error shrinks to by one over square root of five. Hence, it had a much smaller variability that results a different shape.\n",
      "3. Among five \"day of the week\" individual boots, Friday had the highest boot means. In other words, Fridays over the last ten years were probably doing better than the rest weekdays.\n",
      "4. To better see the comparison, notice a cleaner plot underneath the summary plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pyplot.hist(FriBootMean, 25, label=\"Fri\", alpha=.5, normed=True)\n",
      "pyplot.hist(ThrBootMean, 25, label=\"Thr\",  alpha=.5, normed=True)\n",
      "pyplot.hist(WedBootMean, 25, label=\"Wed\",  alpha=.5, normed=True)\n",
      "pyplot.hist(TueBootMean, 25, label=\"Tue\",  alpha=.5, normed=True)\n",
      "pyplot.hist(MonBootMean, 25, label=\"Mon\",  alpha=0.5,normed=True)\n",
      "pyplot.hist(BootMean, 25, label=\"Pooled\",  alpha=.5, normed=True)\n",
      "plt.title('Zoomed in Bootstrap')\n",
      "plt.ylabel('Normalized Histogram, in 1/1000000 th')\n",
      "plt.xlabel('Overnight Return')\n",
      "plt.legend(loc=0)\n",
      "#plt.ylim((0,300000))\n",
      "pyplot.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pyplot.hist(FriBootMean, 25, label=\"Fri\", alpha=.5, normed=True)\n",
      "pyplot.hist(ThrBootMean, 25, label=\"Thr\", alpha=.5, normed=True)\n",
      "#pyplot.hist(WedBootMean, 25, label=\"Wed\", alpha=.5, normed=True)\n",
      "#pyplot.hist(TueBootMean, 25, label=\"Tue\", alpha=.5, normed=True)\n",
      "#pyplot.hist(MonBootMean, 25, label=\"Mon\", alpha=0.5,normed=True)\n",
      "pyplot.hist(BootMean, 25, label=\"Pooled\", alpha=.5, normed=True)\n",
      "plt.title('Reduced Bootstrap')\n",
      "plt.ylabel('Normalized Histogram, in 1/1000000 th')\n",
      "plt.xlabel('Overnight Return')\n",
      "plt.legend(loc=0)\n",
      "plt.ylim((0,300000))\n",
      "pyplot.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Conclusions and Limitations\n",
      "\n",
      "By now via conventional statistical methods, our group has shown that Friday evenings over the past decade were not necessarily resulting a signficiant lower return. However, we acknowledge there are many aggressive assumptions - and perhaps, loopholes - in our research.\n",
      "        \n",
      "The main difficulty of the research was that we utilized conventional statistical methods on a time-series finanical data. Oftentimes, as seen eariler, the methods require each comparison group to possess certain properties, such as normality, independence among each observations, IID, and pairwise movements. However, we perhaps shall assume none of those.\n",
      "        \n",
      "The bootstrapping method was distribution free, meaning that if the data were correctly accessed, the bootstrap summary yet predicts some valuable information of the data structure, regardless correlation within the data set. It is our hope that our audience, at this point, are somewhat convinced that the Weekend Effect might be just a myth."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}